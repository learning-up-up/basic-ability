# 决策树

一种基本的分类与回归方法。主要包括三步：特征选择、决策树的生成、决策树的修剪

## 模型

通过树形结构来描述分类的结果，其中内部节点表示特征，叶子节点代表一个类。

从两个角度来看决策树：

* 每一条根节点到叶子节点的路径表示了一个if-then的逻辑结构，所有的路径构成的集合完备的表示了每一个实例被分类的过程。
* 从条件概率的角度来看，所有路径过程的集合实现了对于特征空间的划分。决策树刻画了不同实例属于不同类别的条件概率分布。最终由联合概率分布的大小决定不同实例的类别。

### 学习过程

我们的目标是得到一个可以较好地对于训练数据进行分类描述地决策树，同时还有具备一定的泛化能力。往往通过正则化的损失函数来描述这一目标

递归的选择最优的特征对于特征空间进行分割，直至没有可再选择的特征或者分类已经足够满意。然后对于已有的树进行剪枝，提高其泛化能力

## 特征选择

用于分类的特征一定要是能对于训练数据进行有效分类的特征。如何评价一个分类是否有效，可以引入**信息增益**进行描述

### 信息增益

首先引入熵的概念。在物理中，熵描述了系统的混乱程度，而在概率论与信息论中，熵作为对于随机变量不确定性的度量。

对于随机变量$`X`$，若
```math
P(X = x_i) = p_i
```
则定义
```math
H(X) = \sum_{i = 1}^np_i\log p_i
```
特别的，当$`p_i = 0`$时，$`p_i\log p_i = 0`$由于熵完全由随机变量的分布来决定，熵也可以写作是对于分布的函数。

条件熵$`H(Y|X) = \sum p_iH(Y|X = x_i)`$表示在Y在X约束下的不确定程度。

有估计（如最大似然估计）得到的熵称为经验熵。

而信息增益就是用来描述特征X使类Y不确定性减少的程度。显然信息增益越大，一个特征的分类效果越好

**信息增益**，特征D对于训练集A的信息增益定义为
```math
g(D,A) = H(D) - H(D|A)