# 决策树

一种基本的分类与回归方法。主要包括三步：特征选择、决策树的生成、决策树的修剪

分类问题是对抑制标签的学习，而聚类才是自行总结特征。

## 模型

通过树形结构来描述分类的结果，其中内部节点表示特征，叶子节点代表一个类。

从两个角度来看决策树：

* 每一条根节点到叶子节点的路径表示了一个if-then的逻辑结构，所有的路径构成的集合完备的表示了每一个实例被分类的过程。
* 从条件概率的角度来看，所有路径过程的集合实现了对于特征空间的划分。决策树刻画了不同实例属于不同类别的条件概率分布。最终由联合概率分布的大小决定不同实例的类别。

### 学习过程

我们的目标是得到一个可以较好地对于训练数据进行分类描述地决策树，同时还有具备一定的泛化能力。往往通过正则化的损失函数来描述这一目标

递归的选择最优的特征对于特征空间进行分割，直至没有可再选择的特征或者分类已经足够满意。然后对于已有的树进行剪枝，提高其泛化能力

## 特征选择

用于分类的特征一定要是能对于训练数据进行有效分类的特征。如何评价一个分类是否有效，可以引入**信息增益**进行描述

### 信息增益

首先引入熵的概念。在物理中，熵描述了系统的混乱程度，而在概率论与信息论中，熵作为对于随机变量不确定性的度量。

对于随机变量$`X`$，若
```math
P(X = x_i) = p_i
```
则定义
```math
H(X) = -\sum_{i = 1}^np_i\log p_i
```

特别的，当$`p_i = 0`$时，$`p_i\log p_i = 0`$由于熵完全由随机变量的分布来决定，熵也可以写作是对于分布的函数。
也可以写作
```math
H(p) = -\sum_{i = 1}^np_i\log p_i
```
熵越大，随机变量的不确定性就越大，$`0 \leq H(p) \leq \log n`$

**条件熵**$`H(Y|X) = \sum p_iH(Y|X = x_i)`$表示在Y在X约束下的不确定程度。

有估计（如最大似然估计）得到的熵称为经验熵。

而信息增益就是用来描述特征X使类Y不确定性减少的程度。显然信息增益越大，一个特征的分类效果越好

**信息增益**，特征D对于训练集A的信息增益定义为
```math
g(D,A) = H(D) - H(D|A)
```

可以解释为引入A特征是否对于特征集的不确定性有有效的降低，评价了分类的有效性，信息增益大的特征具有更强的分类能力

***根据信息增益进行特征选择***就是选择信息增益最大的特征

**信息增益的算法**：

对于训练集D，K个类将训练集划分为$`C_k`$。对于特征A，有n个取值，将训练集划分为n个子集，可以得到：
```math
H(D) = -\sum_{k = 1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}\\
H(D|A) = \sum_{i = 1}^n\frac{|D_i|}{|D|}H(D_i)
```

**信息增益比**：使用信息增益可能会导致倾向取值较多的特征，可以进行一个归一化
```math
g_R(D|A) = \frac{d(D, A)}{H_A(D)}
```

## 决策树的生成

### ID3算法

从根节点开始，对所有的节点计算可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，对于子节点不断递归这一过程直到所有的特征都被选择完为止。

**算法过程**：
* 若D中所有实例属于同一类，直接返回该类的标签
* 若特征集A为空，直接返回D中实例数最大的类的标签
* 否则计算各特征的信息增益，选择信息增益最大的特征
* 若小于阈值，则也就构建一个单节点树
* 否则对于该特征的所有可能值对于训练集分割，将子集中的实例数最大的类别作为标签构建子节点
* 对于所有子节点递归这一过程

### C4.5

使用信息增益比来进行决策树的生成

